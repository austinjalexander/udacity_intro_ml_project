{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Enron POI Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##general imports and mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import pickle\n",
    "sys.path.append(\"ud120-projects/tools/\")\n",
    "sys.path.append(\"ud120-projects/final_project/\")\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##sklearn imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import test_classifier, dump_classifier_and_data\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "data_dict = pickle.load(open(\"ud120-projects/final_project/final_project_dataset.pkl\", \"r\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##original classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "\tAccuracy: 0.25560\tPrecision: 0.18481\tRecall: 0.79800\tF1: 0.30011\tF2: 0.47968\n",
      "\tTotal predictions: 10000\tTrue positives: 1596\tFalse positives: 7040\tFalse negatives:  404\tTrue negatives:  960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary'] # You will need to use more features\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "clf = GaussianNB()    # Provided to give you a starting point. Try a varity of classifiers.\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script.\n",
    "### Because of the small size of the dataset, the script uses stratified\n",
    "### shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list)\n",
    "\n",
    "### Dump your classifier, dataset, and features_list so \n",
    "### anyone can run/check your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##data-record snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METTS MARK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bonus': 600000,\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'mark.metts@enron.com',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 94299,\n",
       " 'from_messages': 29,\n",
       " 'from_poi_to_this_person': 38,\n",
       " 'from_this_person_to_poi': 1,\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 1740,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 585062,\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 365788,\n",
       " 'shared_receipt_with_poi': 702,\n",
       " 'to_messages': 807,\n",
       " 'total_payments': 1061827,\n",
       " 'total_stock_value': 585062}"
      ]
     },
     "execution_count": 1552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print my_dataset.keys()[0]\n",
    "my_dataset.itervalues().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1553,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of names in data:  (146,)\n",
      "\n",
      "first five names:\n",
      "['METTS MARK' 'BAXTER JOHN C' 'ELLIOTT STEVEN' 'CORDES WILLIAM R'\n",
      " 'HANNON KEVIN P']\n",
      "\n",
      "features:\n",
      "['poi', 'bonus', 'deferral_payments', 'deferred_income', 'director_fees', 'exercised_stock_options', 'expenses', 'from_messages', 'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', 'restricted_stock_deferred', 'salary', 'shared_receipt_with_poi', 'to_messages', 'total_payments', 'total_stock_value']\n"
     ]
    }
   ],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    "names = np.array(my_dataset.keys())\n",
    "print \"number of names in data: \", names.shape\n",
    "print \"\\nfirst five names:\\n\", names[:5]\n",
    "features_list = my_dataset.itervalues().next().keys()\n",
    "features_list.sort()\n",
    "features_list.remove('poi')\n",
    "features_list.insert(0, 'poi')\n",
    "features_list.remove('email_address')\n",
    "print \"\\nfeatures:\\n\", features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##data-format conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI Count:\n",
      "0    128\n",
      "1     18\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94299</td>\n",
       "      <td>29</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1740</td>\n",
       "      <td>585062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365788</td>\n",
       "      <td>702</td>\n",
       "      <td>807</td>\n",
       "      <td>1061827</td>\n",
       "      <td>585062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1200000</td>\n",
       "      <td>1295738</td>\n",
       "      <td>-1386055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6680544</td>\n",
       "      <td>11200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1586055</td>\n",
       "      <td>2660303</td>\n",
       "      <td>3942714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5634343</td>\n",
       "      <td>10623258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>350000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-400729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4890344</td>\n",
       "      <td>78552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12961</td>\n",
       "      <td>1788391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211725</td>\n",
       "      <td>6678735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>651850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>386335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1038185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3117011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5538001</td>\n",
       "      <td>34039</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1617011</td>\n",
       "      <td>11350</td>\n",
       "      <td>853064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>243293</td>\n",
       "      <td>1035</td>\n",
       "      <td>1045</td>\n",
       "      <td>288682</td>\n",
       "      <td>6391065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   poi    bonus  deferral_payments  deferred_income  director_fees  \\\n",
       "0    0   600000                NaN              NaN            NaN   \n",
       "1    0  1200000            1295738         -1386055            NaN   \n",
       "2    0   350000                NaN          -400729            NaN   \n",
       "3    0      NaN                NaN              NaN            NaN   \n",
       "4    1  1500000                NaN         -3117011            NaN   \n",
       "\n",
       "   exercised_stock_options  expenses  from_messages  from_poi_to_this_person  \\\n",
       "0                      NaN     94299             29                       38   \n",
       "1                  6680544     11200            NaN                      NaN   \n",
       "2                  4890344     78552            NaN                      NaN   \n",
       "3                   651850       NaN             12                       10   \n",
       "4                  5538001     34039             32                       32   \n",
       "\n",
       "   from_this_person_to_poi  loan_advances  long_term_incentive    other  \\\n",
       "0                        1            NaN                  NaN     1740   \n",
       "1                      NaN            NaN              1586055  2660303   \n",
       "2                      NaN            NaN                  NaN    12961   \n",
       "3                        0            NaN                  NaN      NaN   \n",
       "4                       21            NaN              1617011    11350   \n",
       "\n",
       "   restricted_stock  restricted_stock_deferred  salary  \\\n",
       "0            585062                        NaN  365788   \n",
       "1           3942714                        NaN  267102   \n",
       "2           1788391                        NaN  170941   \n",
       "3            386335                        NaN     NaN   \n",
       "4            853064                        NaN  243293   \n",
       "\n",
       "   shared_receipt_with_poi  to_messages  total_payments  total_stock_value  \n",
       "0                      702          807         1061827             585062  \n",
       "1                      NaN          NaN         5634343           10623258  \n",
       "2                      NaN          NaN          211725            6678735  \n",
       "3                       58          764             NaN            1038185  \n",
       "4                     1035         1045          288682            6391065  "
      ]
     },
     "execution_count": 1554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert dictionary to pandas dataframe\n",
    "\n",
    "df = pd.DataFrame([entry for entry in my_dataset.itervalues()])\n",
    "df = df.drop('email_address', axis=1)\n",
    "df = df[features_list]\n",
    "#df.dtypes\n",
    "#df.describe()\n",
    "#df.count()\n",
    "df.poi = df.poi.astype('int')\n",
    "df = df.convert_objects(convert_numeric=True)\n",
    "\n",
    "for col in list(df.columns):\n",
    "    df[col] = df[col].round(decimals=3)\n",
    "    \n",
    "print \"POI Count:\\n\", df.poi.value_counts()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.020000e+02</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>1.100000e+02</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>1.250000e+02</td>\n",
       "      <td>1.260000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.123288</td>\n",
       "      <td>2374234.609756</td>\n",
       "      <td>1642674.153846</td>\n",
       "      <td>-1140475.142857</td>\n",
       "      <td>166804.882353</td>\n",
       "      <td>5.987054e+06</td>\n",
       "      <td>108728.915789</td>\n",
       "      <td>608.790698</td>\n",
       "      <td>64.895349</td>\n",
       "      <td>41.232558</td>\n",
       "      <td>41962500.0000</td>\n",
       "      <td>1470361.454545</td>\n",
       "      <td>919064.967742</td>\n",
       "      <td>2.321741e+06</td>\n",
       "      <td>166410.555556</td>\n",
       "      <td>562194.294737</td>\n",
       "      <td>1176.465116</td>\n",
       "      <td>2073.860465</td>\n",
       "      <td>5.081526e+06</td>\n",
       "      <td>6.773957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.329899</td>\n",
       "      <td>10713327.969046</td>\n",
       "      <td>5161929.973575</td>\n",
       "      <td>4025406.378506</td>\n",
       "      <td>319891.409747</td>\n",
       "      <td>3.106201e+07</td>\n",
       "      <td>533534.814109</td>\n",
       "      <td>1841.033949</td>\n",
       "      <td>86.979244</td>\n",
       "      <td>100.073111</td>\n",
       "      <td>47083208.7019</td>\n",
       "      <td>5942759.315498</td>\n",
       "      <td>4589252.907638</td>\n",
       "      <td>1.251828e+07</td>\n",
       "      <td>4201494.314703</td>\n",
       "      <td>2716369.154553</td>\n",
       "      <td>1178.317641</td>\n",
       "      <td>2582.700981</td>\n",
       "      <td>2.906172e+07</td>\n",
       "      <td>3.895777e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>-102500.000000</td>\n",
       "      <td>-27992891.000000</td>\n",
       "      <td>3285.000000</td>\n",
       "      <td>3.285000e+03</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>400000.0000</td>\n",
       "      <td>69223.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-2.604490e+06</td>\n",
       "      <td>-7576788.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1.480000e+02</td>\n",
       "      <td>-4.409300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>431250.000000</td>\n",
       "      <td>81573.000000</td>\n",
       "      <td>-694862.000000</td>\n",
       "      <td>98784.000000</td>\n",
       "      <td>5.278862e+05</td>\n",
       "      <td>22614.000000</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1600000.0000</td>\n",
       "      <td>281250.000000</td>\n",
       "      <td>1215.000000</td>\n",
       "      <td>2.540180e+05</td>\n",
       "      <td>-389621.750000</td>\n",
       "      <td>211816.000000</td>\n",
       "      <td>249.750000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>3.944750e+05</td>\n",
       "      <td>4.945102e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>769375.000000</td>\n",
       "      <td>227449.000000</td>\n",
       "      <td>-159792.000000</td>\n",
       "      <td>108579.000000</td>\n",
       "      <td>1.310814e+06</td>\n",
       "      <td>46950.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>41762500.0000</td>\n",
       "      <td>442035.000000</td>\n",
       "      <td>52382.000000</td>\n",
       "      <td>4.517400e+05</td>\n",
       "      <td>-146975.000000</td>\n",
       "      <td>259996.000000</td>\n",
       "      <td>740.500000</td>\n",
       "      <td>1211.000000</td>\n",
       "      <td>1.101393e+06</td>\n",
       "      <td>1.102872e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200000.000000</td>\n",
       "      <td>1002671.500000</td>\n",
       "      <td>-38346.000000</td>\n",
       "      <td>113784.000000</td>\n",
       "      <td>2.547724e+06</td>\n",
       "      <td>79952.500000</td>\n",
       "      <td>145.500000</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>82125000.0000</td>\n",
       "      <td>938672.000000</td>\n",
       "      <td>362096.000000</td>\n",
       "      <td>1.002370e+06</td>\n",
       "      <td>-75009.750000</td>\n",
       "      <td>312117.000000</td>\n",
       "      <td>1888.250000</td>\n",
       "      <td>2634.750000</td>\n",
       "      <td>2.093263e+06</td>\n",
       "      <td>2.949847e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>97343619.000000</td>\n",
       "      <td>32083396.000000</td>\n",
       "      <td>-833.000000</td>\n",
       "      <td>1398517.000000</td>\n",
       "      <td>3.117640e+08</td>\n",
       "      <td>5235198.000000</td>\n",
       "      <td>14368.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>83925000.0000</td>\n",
       "      <td>48521928.000000</td>\n",
       "      <td>42667589.000000</td>\n",
       "      <td>1.303223e+08</td>\n",
       "      <td>15456290.000000</td>\n",
       "      <td>26704229.000000</td>\n",
       "      <td>5521.000000</td>\n",
       "      <td>15149.000000</td>\n",
       "      <td>3.098866e+08</td>\n",
       "      <td>4.345095e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              poi            bonus  deferral_payments  deferred_income  \\\n",
       "count  146.000000        82.000000          39.000000        49.000000   \n",
       "mean     0.123288   2374234.609756     1642674.153846  -1140475.142857   \n",
       "std      0.329899  10713327.969046     5161929.973575   4025406.378506   \n",
       "min      0.000000     70000.000000     -102500.000000 -27992891.000000   \n",
       "25%      0.000000    431250.000000       81573.000000   -694862.000000   \n",
       "50%      0.000000    769375.000000      227449.000000   -159792.000000   \n",
       "75%      0.000000   1200000.000000     1002671.500000    -38346.000000   \n",
       "max      1.000000  97343619.000000    32083396.000000      -833.000000   \n",
       "\n",
       "        director_fees  exercised_stock_options        expenses  from_messages  \\\n",
       "count       17.000000             1.020000e+02       95.000000      86.000000   \n",
       "mean    166804.882353             5.987054e+06   108728.915789     608.790698   \n",
       "std     319891.409747             3.106201e+07   533534.814109    1841.033949   \n",
       "min       3285.000000             3.285000e+03      148.000000      12.000000   \n",
       "25%      98784.000000             5.278862e+05    22614.000000      22.750000   \n",
       "50%     108579.000000             1.310814e+06    46950.000000      41.000000   \n",
       "75%     113784.000000             2.547724e+06    79952.500000     145.500000   \n",
       "max    1398517.000000             3.117640e+08  5235198.000000   14368.000000   \n",
       "\n",
       "       from_poi_to_this_person  from_this_person_to_poi  loan_advances  \\\n",
       "count                86.000000                86.000000         4.0000   \n",
       "mean                 64.895349                41.232558  41962500.0000   \n",
       "std                  86.979244               100.073111  47083208.7019   \n",
       "min                   0.000000                 0.000000    400000.0000   \n",
       "25%                  10.000000                 1.000000   1600000.0000   \n",
       "50%                  35.000000                 8.000000  41762500.0000   \n",
       "75%                  72.250000                24.750000  82125000.0000   \n",
       "max                 528.000000               609.000000  83925000.0000   \n",
       "\n",
       "       long_term_incentive            other  restricted_stock  \\\n",
       "count            66.000000        93.000000      1.100000e+02   \n",
       "mean        1470361.454545    919064.967742      2.321741e+06   \n",
       "std         5942759.315498   4589252.907638      1.251828e+07   \n",
       "min           69223.000000         2.000000     -2.604490e+06   \n",
       "25%          281250.000000      1215.000000      2.540180e+05   \n",
       "50%          442035.000000     52382.000000      4.517400e+05   \n",
       "75%          938672.000000    362096.000000      1.002370e+06   \n",
       "max        48521928.000000  42667589.000000      1.303223e+08   \n",
       "\n",
       "       restricted_stock_deferred           salary  shared_receipt_with_poi  \\\n",
       "count                  18.000000        95.000000                86.000000   \n",
       "mean               166410.555556    562194.294737              1176.465116   \n",
       "std               4201494.314703   2716369.154553              1178.317641   \n",
       "min              -7576788.000000       477.000000                 2.000000   \n",
       "25%               -389621.750000    211816.000000               249.750000   \n",
       "50%               -146975.000000    259996.000000               740.500000   \n",
       "75%                -75009.750000    312117.000000              1888.250000   \n",
       "max              15456290.000000  26704229.000000              5521.000000   \n",
       "\n",
       "        to_messages  total_payments  total_stock_value  \n",
       "count     86.000000    1.250000e+02       1.260000e+02  \n",
       "mean    2073.860465    5.081526e+06       6.773957e+06  \n",
       "std     2582.700981    2.906172e+07       3.895777e+07  \n",
       "min       57.000000    1.480000e+02      -4.409300e+04  \n",
       "25%      541.250000    3.944750e+05       4.945102e+05  \n",
       "50%     1211.000000    1.101393e+06       1.102872e+06  \n",
       "75%     2634.750000    2.093263e+06       2.949847e+06  \n",
       "max    15149.000000    3.098866e+08       4.345095e+08  "
      ]
     },
     "execution_count": 1555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###separate labels from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146,)\n",
      "[0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# create labels\n",
    "y = df.poi.values\n",
    "print y.shape\n",
    "print y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 19)\n",
      "[[  6.00000000e+05              nan              nan              nan\n",
      "               nan   9.42990000e+04   2.90000000e+01   3.80000000e+01\n",
      "    1.00000000e+00              nan              nan   1.74000000e+03\n",
      "    5.85062000e+05              nan   3.65788000e+05   7.02000000e+02\n",
      "    8.07000000e+02   1.06182700e+06   5.85062000e+05]\n",
      " [  1.20000000e+06   1.29573800e+06  -1.38605500e+06              nan\n",
      "    6.68054400e+06   1.12000000e+04              nan              nan\n",
      "               nan              nan   1.58605500e+06   2.66030300e+06\n",
      "    3.94271400e+06              nan   2.67102000e+05              nan\n",
      "               nan   5.63434300e+06   1.06232580e+07]\n",
      " [  3.50000000e+05              nan  -4.00729000e+05              nan\n",
      "    4.89034400e+06   7.85520000e+04              nan              nan\n",
      "               nan              nan              nan   1.29610000e+04\n",
      "    1.78839100e+06              nan   1.70941000e+05              nan\n",
      "               nan   2.11725000e+05   6.67873500e+06]]\n"
     ]
    }
   ],
   "source": [
    "# create initial features\n",
    "X = df.drop('poi', axis=1).values\n",
    "print X.shape\n",
    "print X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##'NaN' imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.00000000e+05   2.27449000e+05  -1.59792000e+05   1.08579000e+05\n",
      "    1.31081350e+06   9.42990000e+04   2.90000000e+01   3.80000000e+01\n",
      "    1.00000000e+00   4.17625000e+07   4.42035000e+05   1.74000000e+03\n",
      "    5.85062000e+05  -1.46975000e+05   3.65788000e+05   7.02000000e+02\n",
      "    8.07000000e+02   1.06182700e+06   5.85062000e+05]\n",
      " [  1.20000000e+06   1.29573800e+06  -1.38605500e+06   1.08579000e+05\n",
      "    6.68054400e+06   1.12000000e+04   4.10000000e+01   3.50000000e+01\n",
      "    8.00000000e+00   4.17625000e+07   1.58605500e+06   2.66030300e+06\n",
      "    3.94271400e+06  -1.46975000e+05   2.67102000e+05   7.40500000e+02\n",
      "    1.21100000e+03   5.63434300e+06   1.06232580e+07]\n",
      " [  3.50000000e+05   2.27449000e+05  -4.00729000e+05   1.08579000e+05\n",
      "    4.89034400e+06   7.85520000e+04   4.10000000e+01   3.50000000e+01\n",
      "    8.00000000e+00   4.17625000e+07   4.42035000e+05   1.29610000e+04\n",
      "    1.78839100e+06  -1.46975000e+05   1.70941000e+05   7.40500000e+02\n",
      "    1.21100000e+03   2.11725000e+05   6.67873500e+06]]\n"
     ]
    }
   ],
   "source": [
    "# impute 'NaN' values to column means\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "print X[:3]\n",
    "\n",
    "imp_values = imp.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 19)\n",
      "names associated with outlier-containing rows to remove:\n",
      "\tFREVERT MARK A index: 129  (poi? 0)\n",
      "\tMARTIN AMANDA K index: 139  (poi? 0)\n",
      "\tGAHN ROBERT S index: 70  (poi? 0)\n",
      "\tLAY KENNETH L index: 65  (poi? 1)\n",
      "\tTOTAL index: 104  (poi? 0)\n",
      "\tSKILLING JEFFREY K index: 95  (poi? 1)\n",
      "\tLAVORATO JOHN J index: 43  (poi? 0)\n",
      "\tBOWEN JR RAYMOND M index: 76  (poi? 1)\n",
      "\tBELDEN TIMOTHY N index: 82  (poi? 1)\n",
      "\tSHAPIRO RICHARD S index: 51  (poi? 0)\n",
      "\tBHATNAGAR SANJAY index: 118  (poi? 0)\n",
      "\tBELFER ROBERT index: 24  (poi? 0)\n",
      "\tKAMINSKI WINCENTY J index: 89  (poi? 0)\n",
      "\tMCCLELLAN GEORGE index: 71  (poi? 0)\n",
      "\tKEAN STEVEN J index: 103  (poi? 0)\n",
      "\tDELAINEY DAVID W index: 60  (poi? 1)\n",
      "\tDIETRICH JANET R index: 127  (poi? 0)\n"
     ]
    }
   ],
   "source": [
    "### Task 2: Remove outliers\n",
    "\n",
    "# hand-tuned to remove ~5% (in this case, 7%)\n",
    "num_rows = X.shape[0]\n",
    "num_cols = X.shape[1]\n",
    "rows_to_remove = set()\n",
    "\n",
    "for i in xrange(num_cols):\n",
    "    ninety_nine_point_nine_percentile = np.percentile(X[:,i], 99)\n",
    "    \n",
    "    for j in xrange(num_rows):\n",
    "        if X[j,i] > ninety_nine_point_nine_percentile:\n",
    "            rows_to_remove.add(j)\n",
    "\n",
    "print X.shape\n",
    "\n",
    "print \"names associated with outlier-containing rows to remove:\"\n",
    "for i in rows_to_remove:\n",
    "    print \"\\t\",names[i], \"index: {}\".format(i), \" (poi? {})\".format(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new X shape:  (145, 19)\n",
      "\n",
      "new y shape:  (145,)\n",
      "\n",
      "total rows removed:  1 (0.12)\n"
     ]
    }
   ],
   "source": [
    "X = np.delete(X, 104, axis=0)\n",
    "y = np.delete(y, 104)\n",
    "    \n",
    "names = np.delete(names, 104)\n",
    "\n",
    "print \"\\nnew X shape: \", X.shape\n",
    "print \"\\nnew y shape: \", y.shape\n",
    "\n",
    "print \"\\ntotal rows removed: \", 1, \"({})\".format(round(len(rows_to_remove)/float(num_rows), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1561,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "\n",
    "# select K best to explore feature engineering possibilities\n",
    "selector = SelectKBest().fit(X, y)\n",
    "\n",
    "def selectkbest(selector):\n",
    "\n",
    "    features = features_list[1:]\n",
    "\n",
    "    high_scores = []\n",
    "\n",
    "    print \"SelectKBest SCORES:\"\n",
    "    selectkbest_scores = np.round(selector.scores_, 2)\n",
    "    for i in xrange(len(features)):\n",
    "        print \"\\t\", features[i], \": \", selectkbest_scores[i]\n",
    "        if selectkbest_scores[i] > 2:\n",
    "            high_scores.append(i)\n",
    "\n",
    "    print \"\\nSelectKBest HIGH SCORES:\"\n",
    "    for i in high_scores:\n",
    "        print \"\\t\", features[i], \"[{}]\".format(i), \": \", selectkbest_scores[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest SCORES:\n",
      "\tbonus :  15.95\n",
      "\tdeferral_payments :  0.26\n",
      "\tdeferred_income :  10.41\n",
      "\tdirector_fees :  0.42\n",
      "\texercised_stock_options :  27.7\n",
      "\texpenses :  1.0\n",
      "\tfrom_messages :  0.18\n",
      "\tfrom_poi_to_this_person :  4.34\n",
      "\tfrom_this_person_to_poi :  2.26\n",
      "\tloan_advances :  3.86\n",
      "\tlong_term_incentive :  8.45\n",
      "\tother :  3.97\n",
      "\trestricted_stock :  8.56\n",
      "\trestricted_stock_deferred :  0.1\n",
      "\tsalary :  10.99\n",
      "\tshared_receipt_with_poi :  7.57\n",
      "\tto_messages :  0.91\n",
      "\ttotal_payments :  8.51\n",
      "\ttotal_stock_value :  23.91\n",
      "\n",
      "SelectKBest HIGH SCORES:\n",
      "\tbonus [0] :  15.95\n",
      "\tdeferred_income [2] :  10.41\n",
      "\texercised_stock_options [4] :  27.7\n",
      "\tfrom_poi_to_this_person [7] :  4.34\n",
      "\tfrom_this_person_to_poi [8] :  2.26\n",
      "\tloan_advances [9] :  3.86\n",
      "\tlong_term_incentive [10] :  8.45\n",
      "\tother [11] :  3.97\n",
      "\trestricted_stock [12] :  8.56\n",
      "\tsalary [14] :  10.99\n",
      "\tshared_receipt_with_poi [15] :  7.57\n",
      "\ttotal_payments [17] :  8.51\n",
      "\ttotal_stock_value [18] :  23.91\n"
     ]
    }
   ],
   "source": [
    "selectkbest(selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_new_feature(X, col1, col2, operation, feature_name):\n",
    "    \n",
    "    features_list.append(feature_name)\n",
    "    \n",
    "    new_col = []\n",
    "    if operation == '*':\n",
    "        new_col = (X[:,col1] * X[:,col2])\n",
    "    elif operation == '/':\n",
    "        new_col = np.true_divide(X[:,col1], X[:, col2])\n",
    "    \n",
    "    new_col.shape = (new_col.shape[0], 1)\n",
    "    #print new_col.shape\n",
    "\n",
    "    X = np.hstack((X, new_col))\n",
    "    #print X.shape\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelectKBest SCORES:\n",
      "\tbonus :  15.95\n",
      "\tdeferral_payments :  0.26\n",
      "\tdeferred_income :  10.41\n",
      "\tdirector_fees :  0.42\n",
      "\texercised_stock_options :  27.7\n",
      "\texpenses :  1.0\n",
      "\tfrom_messages :  0.18\n",
      "\tfrom_poi_to_this_person :  4.34\n",
      "\tfrom_this_person_to_poi :  2.26\n",
      "\tloan_advances :  3.86\n",
      "\tlong_term_incentive :  8.45\n",
      "\tother :  3.97\n",
      "\trestricted_stock :  8.56\n",
      "\trestricted_stock_deferred :  0.1\n",
      "\tsalary :  10.99\n",
      "\tshared_receipt_with_poi :  7.57\n",
      "\tto_messages :  0.91\n",
      "\ttotal_payments :  8.51\n",
      "\ttotal_stock_value :  23.91\n",
      "\tperc_of_emails_to_poi :  12.95\n",
      "\n",
      "SelectKBest HIGH SCORES:\n",
      "\tbonus [0] :  15.95\n",
      "\tdeferred_income [2] :  10.41\n",
      "\texercised_stock_options [4] :  27.7\n",
      "\tfrom_poi_to_this_person [7] :  4.34\n",
      "\tfrom_this_person_to_poi [8] :  2.26\n",
      "\tloan_advances [9] :  3.86\n",
      "\tlong_term_incentive [10] :  8.45\n",
      "\tother [11] :  3.97\n",
      "\trestricted_stock [12] :  8.56\n",
      "\tsalary [14] :  10.99\n",
      "\tshared_receipt_with_poi [15] :  7.57\n",
      "\ttotal_payments [17] :  8.51\n",
      "\ttotal_stock_value [18] :  23.91\n",
      "\tperc_of_emails_to_poi [19] :  12.95\n"
     ]
    }
   ],
   "source": [
    "X = create_new_feature(X, 8, 6, '/', 'perc_of_emails_to_poi')\n",
    "selector = SelectKBest().fit(X, y)\n",
    "selectkbest(selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UDACITY TESTER RESULTS: \n",
      "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomizedpca', RandomizedPCA(copy=True, iterated_power=1, n_components=8, random_state=42,\n",
      "       whiten=True)), ('decisiontreeclassifier', DecisionTreeClassifier(class_weight='auto', criterion='entropy',\n",
      "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, random_state=42, splitter='best'))])\n",
      "\tAccuracy: 0.80580\tPrecision: 0.24821\tRecall: 0.22500\tF1: 0.23603\tF2: 0.22929\n",
      "\tTotal predictions: 15000\tTrue positives:  450\tFalse positives: 1363\tFalse negatives: 1550\tTrue negatives: 11637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "skb = SelectKBest(k=4).fit(X, y)\n",
    "X = skb.transform(X)\n",
    "\n",
    "features = features_list[1:]\n",
    "selectkbest_scores = zip(np.round(skb.scores_, 2), [i for i in xrange(len(skb.scores_))])\n",
    "feature_indices_to_remove = [x[1] for x in sorted(selectkbest_scores, reverse=True)[k:]]\n",
    "features_to_remove = []\n",
    "for i in feature_indices_to_remove:\n",
    "    features_to_remove.append(features[i])\n",
    "\n",
    "# prepare for Udacity tester\n",
    "\n",
    "# remove features from feature_list\n",
    "for feature in features_to_remove:\n",
    "    if feature in my_dataset:\n",
    "        my_dataset[key].pop(feature)\n",
    "    if feature in features_list:\n",
    "        features_list.remove(feature)\n",
    "'''\n",
    "\n",
    "# remove email_address from my_dataset\n",
    "for key in my_dataset.keys():\n",
    "    my_dataset[key].pop('email_address')\n",
    "\n",
    "\n",
    "# remove outliers from original data set\n",
    "for key in my_dataset.keys():\n",
    "    if key not in names:\n",
    "        my_dataset.pop(key)\n",
    "\n",
    "# replace 'NaN's\n",
    "for key in my_dataset.keys():\n",
    "    for sub_key in my_dataset[key].keys():\n",
    "        if my_dataset[key][sub_key] == 'NaN':\n",
    "            i = (df.columns.get_loc(sub_key) - 1)\n",
    "            my_dataset[key][sub_key] = imp_values[i]\n",
    "\n",
    "# add created feature\n",
    "i = 0\n",
    "for key in my_dataset.keys():\n",
    "    my_dataset[key]['perc_of_emails_to_poi'] = X[i,-1]\n",
    "    i += 1\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), RandomizedPCA(), clf)\n",
    "#pipeline = make_pipeline(StandardScaler(), clf)\n",
    "\n",
    "#params = dict(randomizedpca__n_components = [2, 4, 6, 8, 10],\n",
    "#              randomizedpca__iterated_power = [1, 2, 3],\n",
    "#              randomizedpca__whiten = [True, False],\n",
    "#              randomizedpca__random_state = [42])\n",
    "\n",
    "params = dict(randomizedpca__n_components = [2, 4, 6, 8, 10],\n",
    "              randomizedpca__iterated_power = [1, 2, 3],\n",
    "              randomizedpca__whiten = [True, False],\n",
    "              randomizedpca__random_state = [42],\n",
    "              decisiontreeclassifier__criterion = ['gini', 'entropy'],\n",
    "              decisiontreeclassifier__max_features = ['auto', 'sqrt', 'log2', None],\n",
    "              decisiontreeclassifier__class_weight = ['auto', None],\n",
    "              decisiontreeclassifier__random_state = [42]) \n",
    "\n",
    "#params = dict(decisiontreeclassifier__criterion = ['gini', 'entropy'],\n",
    "#              decisiontreeclassifier__max_features = ['auto', 'sqrt', 'log2', None],\n",
    "#              decisiontreeclassifier__class_weight = ['auto', None],\n",
    "#              decisiontreeclassifier__random_state = [42]) \n",
    "\n",
    "# cross validation    \n",
    "cv = StratifiedShuffleSplit(y, test_size=0.2, random_state=42)\n",
    "\n",
    "# tune parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid=params, n_jobs=1, cv=cv)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "clf = grid_search.best_estimator_\n",
    "\n",
    "# use Udacity tester\n",
    "print \"\\nUDACITY TESTER RESULTS: \"\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(X)\n",
    "corr_df['label'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31678588,  0.40282356,  0.37850901,  0.28817233])"
      ]
     },
     "execution_count": 1488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_df.corr()['label'].values[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = corr_df.corr()['label'].values[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.316785880785\n",
      "1 0.40282356443\n",
      "2 0.378509012403\n",
      "3 0.28817232998\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(len(corr)):\n",
    "    if abs(corr[i]) > 0.2:\n",
    "        print i, abs(corr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##machine learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "def grid_searcher(clf, pca_skb, output):\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    even_range = range(2,X.shape[1],2)\n",
    "    random_state = [42]\n",
    "    t_or_f = [True, False]\n",
    "    #powers_of_ten = [10**x for x in range(-5,5)]\n",
    "    logspace = np.logspace(-5, 5, 10)\n",
    "    #kernels = ['linear', 'poly', 'rbf', 'sigmoid']  # takes too long, unfortunately\n",
    "    kernels = ['rbf']\n",
    "    criteria = ['gini', 'entropy']\n",
    "    splitters = ['best', 'random']\n",
    "    max_features = ['auto', 'sqrt', 'log2', None]\n",
    "    \n",
    "    # modify features, remove features via pipeline\n",
    "    \n",
    "    pipeline = []\n",
    "    params = dict()\n",
    "    pipeline_clf = \"\"\n",
    "    \n",
    "    if pca_skb == \"pca_skb\":\n",
    "        #pipeline = make_pipeline(MinMaxScaler(), make_union(RandomizedPCA(), SelectKBest()), clf)\n",
    "        pipeline = make_pipeline(StandardScaler(), make_union(RandomizedPCA(), SelectKBest()), clf)\n",
    "\n",
    "        params = dict(featureunion__randomizedpca__n_components = even_range,\n",
    "                      featureunion__randomizedpca__iterated_power = [1, 2, 3],\n",
    "                      featureunion__randomizedpca__whiten = t_or_f,\n",
    "                      featureunion__randomizedpca__random_state = random_state,\n",
    "                      featureunion__selectkbest__k = even_range)   \n",
    "        \n",
    "    elif pca_skb == \"pca\":\n",
    "        #pipeline = make_pipeline(MinMaxScaler(), RandomizedPCA(), clf)\n",
    "        pipeline = make_pipeline(StandardScaler(), RandomizedPCA(), clf)\n",
    "\n",
    "        params = dict(randomizedpca__n_components = [4],\n",
    "                      randomizedpca__iterated_power = [1, 2, 3],\n",
    "                      randomizedpca__whiten = t_or_f,\n",
    "                      randomizedpca__random_state = random_state)   \n",
    "        \n",
    "    elif pca_skb == \"skb\":\n",
    "        #pipeline = make_pipeline(MinMaxScaler(), SelectKBest(), clf)\n",
    "        pipeline = make_pipeline(StandardScaler(), SelectKBest(), clf)\n",
    "\n",
    "        params = dict(selectkbest__k = [4])   \n",
    "    \n",
    "    pipeline_clf = pipeline.steps[2][0]      \n",
    "    \n",
    "    if pipeline_clf == 'decisiontreeclassifier' or pipeline_clf == 'randomforestclassifier':\n",
    "        params[\"{}__criterion\".format(pipeline_clf)] = criteria\n",
    "        #params[\"{}__splitter\".format(pipeline_clf)] = splitters\n",
    "        params[\"{}__max_features\".format(pipeline_clf)] = max_features\n",
    "        #params[\"{}__min_samples_split\".format(pipeline_clf)] = even_range\n",
    "        params[\"{}__class_weight\".format(pipeline_clf)] = ['auto', None]\n",
    "        params[\"{}__random_state\".format(pipeline_clf)] = random_state\n",
    "    \n",
    "    if pipeline_clf == 'svc':\n",
    "        params['svc__C'] = logspace\n",
    "        params['svc__kernel'] = kernels\n",
    "        #params['svc__degree'] = [1,2,3,4,5]  # for use with 'poly'\n",
    "        params['svc__gamma'] = logspace\n",
    "        params['svc__random_state'] = random_state\n",
    "        \n",
    "    # cross validation    \n",
    "    cv = StratifiedShuffleSplit(y, test_size=0.2, random_state=random_state[0])\n",
    "    \n",
    "    # tune parameters\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=params, n_jobs=1, cv=cv)\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    if output == True:\n",
    "        print \"*\"*15, pipeline_clf.upper(), \"*\"*15\n",
    "        print \"\\nBEST SCORE: \", grid_search.best_score_, \"\\n\"\n",
    "        print \"\\nBEST PARAMS: \", grid_search.best_params_, \"\\n\"\n",
    "\n",
    "    # split into training and testing data for reporting results\n",
    "    if output == True:\n",
    "        print \"#\"*50\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state[0])\n",
    "\n",
    "    if output == True:\n",
    "        print \"\\nBEST ESTIMATOR:\"\n",
    "    clf = grid_search.best_estimator_\n",
    "    if output == True:\n",
    "        print clf\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    if pca_skb == \"skb\" or pca_skb == \"pca_skb\":\n",
    "    \n",
    "        if output == True:\n",
    "            print \"\\nSelectKBest SCORES:\"\n",
    "        features = features_list[1:]\n",
    "        \n",
    "        selectkbest_scores = clf.steps[1][1].scores_ if pca_skb == \"skb\" else clf.steps[1][1].transformer_list[1][1].scores_\n",
    "        \n",
    "        selectkbest_scores = np.round(selectkbest_scores, 2)\n",
    "        for i in xrange(len(features)):\n",
    "            if output == True:\n",
    "                print \"\\t\", features[i], \": \", selectkbest_scores[i]\n",
    "    \n",
    "    if pipeline_clf == 'decisiontreeclassifier' or pipeline_clf == 'randomforestclassifier':\n",
    "        if output == True:\n",
    "            print \"\\n{} FEATURE IMPORTANCES:\".format(pipeline_clf.upper())\n",
    "            print clf.steps[2][1].feature_importances_\n",
    "    \n",
    "    if output == True:\n",
    "        print \"\\n\", \"#\"*50\n",
    "    \n",
    "        print \"\\nPREDICTIONS:\"\n",
    "\n",
    "        print \"\\nground truth:\\n\\t\", y_test \n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    if output == True:\n",
    "        print \"\\npredictions:\\n\\t\", y_pred\n",
    "\n",
    "        print \"\\nscore: \", clf.score(X_test, y_test)\n",
    "\n",
    "        print \"\\nEVALUATIONS:\"\n",
    "        print \"\\nconfusion matrix:\\n\", confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "        print \"\\nclassification report:\\n\", classification_report(y_test, y_pred, target_names=[\"non-poi\", \"poi\"])\n",
    "    \n",
    "        print \"ELAPSED TIME: \", round(time()-t0,3), \"s\"\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##reminder: original classifier results\n",
    "\n",
    "GaussianNB()  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Accuracy: 0.25560&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Precision: 0.18481&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Recall: 0.79800&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F1: 0.30011&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F2: 0.47968  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total predictions: 10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;True positives: 1596&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False positives: 7040&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;False negatives:  404  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;True negatives:  960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'email_address'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1492-dc542dbb6ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# remove emails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmy_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'email_address'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# remove outliers from original data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'email_address'"
     ]
    }
   ],
   "source": [
    "# prepare for Udacity tester\n",
    "\n",
    "# remove emails\n",
    "for key in my_dataset.keys():\n",
    "    my_dataset[key].pop('email_address')\n",
    "    \n",
    "# remove outliers from original data set\n",
    "for key in my_dataset.keys():\n",
    "    if key not in names:\n",
    "        my_dataset.pop(key)\n",
    "\n",
    "# replace 'NaN's\n",
    "for key in my_dataset.keys():\n",
    "    for sub_key in my_dataset[key].keys():\n",
    "        if my_dataset[key][sub_key] == 'NaN':\n",
    "            i = (df.columns.get_loc(sub_key) - 1)\n",
    "            my_dataset[key][sub_key] = imp_values[i]\n",
    "            \n",
    "# add created feature\n",
    "i = 0\n",
    "for key in my_dataset.keys():\n",
    "    my_dataset[key]['selectkbest_product'] = X[i,-1]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def udacity_tester():\n",
    "    # use Udacity tester\n",
    "    print \"\\nUDACITY TESTER RESULTS: \"\n",
    "    test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##new classifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for classifier in [GaussianNB(), DecisionTreeClassifier()]:\n",
    "    for transfomer in ['pca', 'skb', 'pca_skb']:\n",
    "        clf = grid_searcher(classifier, transfomer, output=True)\n",
    "        udacity_tester()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##data dump for Udacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##average best-scoring predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clfs = dict()\n",
    "\n",
    "for classifier in [GaussianNB(), DecisionTreeClassifier()]:\n",
    "\n",
    "    clfs[str(classifier)] = dict()\n",
    "    \n",
    "    for transformer in ['pca', 'skb', 'pca_skb']:\n",
    "        clf = grid_searcher(classifier, transformer, output=False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        clfs[str(classifier)][transformer] = dict()\n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        clfs[str(classifier)][transformer]['predictions'] = y_pred\n",
    "        \n",
    "        clf_report = classification_report(y_test, y_pred, target_names=[\"non-poi\", \"poi\"])\n",
    "        clfs[str(classifier)][transformer]['clf_report'] = clf_report\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs['GaussianNB()']['pca']['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs['GaussianNB()']['pca']['clf_report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_predictions = dict()\n",
    "for clsfr in clfs.keys():\n",
    "    best_predictions[clsfr] = dict()\n",
    "    #print clsfr\n",
    "    best_f1_score = 0.0\n",
    "    for transformer in clfs[clsfr].keys():\n",
    "        clf_report = clfs[clsfr][transformer]['clf_report']\n",
    "        clf_report = [re.sub(r\"[a-z]|\\n\", '', x) for x in clf_report.split(\" \")]\n",
    "        clf_report = filter(None, clf_report)\n",
    "        #print \"\\t\", transformer\n",
    "        f1_score = float(clf_report[-2])\n",
    "        #print f1_score\n",
    "        if f1_score > best_f1_score:\n",
    "            best_f1_score = f1_score\n",
    "            best_predictions[clsfr]['transformer'] = transformer\n",
    "            best_predictions[clsfr]['f1_score'] = f1_score\n",
    "            best_predictions[clsfr]['predictions'] = clfs[clsfr][transformer]['predictions']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "averaged_best_recall_predictions = np.zeros((28,))\n",
    "\n",
    "for clsfr in best_predictions.keys():\n",
    "    print clsfr[:10]\n",
    "    print \"\\tbest transformer: \", best_predictions[clsfr]['transformer']\n",
    "    print \"\\tbest f1 score: \", best_predictions[clsfr]['f1_score']\n",
    "    print \"\\tbest predictions: \", best_predictions[clsfr]['predictions']\n",
    "    \n",
    "    averaged_best_recall_predictions = np.maximum(averaged_best_recall_predictions, best_predictions[clsfr]['predictions'])\n",
    "    \n",
    "print \"\\naveraged best predictions: {}\".format(averaged_best_recall_predictions.astype('int'))\n",
    "\n",
    "print \"\\nresulting confusion matrix:\\n\", confusion_matrix(y_test, averaged_best_recall_predictions)\n",
    "\n",
    "print \"\\nresulting classification report:\\n\", classification_report(y_test, averaged_best_recall_predictions, target_names=[\"non-poi\", \"poi\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Epilogue: email text as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had hoped to use email-text data along with the financial data, but clearly there is a discrepancy between the inidividuals represented by the financial data and the email data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare individuals represented by the financial data and by the email-corpus data\n",
    "\n",
    "directory_names = []\n",
    "poi_directory_names = []\n",
    "true_count = 0\n",
    "false_count = 0\n",
    "\n",
    "for key in my_dataset.keys():\n",
    "    names = key.lower().split(' ')\n",
    "    dirname = names.pop(0)\n",
    "    \n",
    "    if len(names) > 0:\n",
    "        dirname = dirname + \"-\" + names[0][0]\n",
    "\n",
    "    exist = os.path.exists('/Users/excalibur/Dropbox/datasets/maildir/{}'.format(dirname))\n",
    "    \n",
    "    #print dirname, \"\\n\\temails exist: \", exist, names\n",
    "    \n",
    "    directory_names.append(dirname)\n",
    "    \n",
    "    if exist:\n",
    "        true_count += 1\n",
    "        if my_dataset[key]['poi'] == True:\n",
    "            poi_directory_names.append(dirname)\n",
    "    else:\n",
    "        false_count += 1\n",
    "        \n",
    "print \"email directories matching individuals represented by financial data:\"\n",
    "print \"\\texist: \", true_count, \"(POIs: {})\".format(len(poi_directory_names))\n",
    "print \"\\tdon't exist: \", false_count\n",
    "\n",
    "#print sorted(directory_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it seems clear that it would be difficult to join the two data sets in a meaningful way due to their lack of overlap. \n",
    "\n",
    "After this project, I may spend some time with text-vectorization of the email corpus and examine things like word frequencies (I have started working on some of the initial code for that process below); however, with such an apparent low number of known POI-emails being available (only 3!?), it's entirely unclear how useful such an endeavor would be for identifying POIs, although there are surely other interesting insights to be gleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "email_corpus_dir = '/Users/excalibur/Dropbox/datasets/maildir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email_dirs = os.listdir(email_corpus_dir)\n",
    "print \"number of email directories: \", len(email_dirs)\n",
    "print \"\\nfirst five email directories:\\n\", email_dirs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email_text = dict()\n",
    "for email_dir in email_dirs:\n",
    "    for dirpath, dirnames, filenames in os.walk(email_corpus_dir + email_dir):\n",
    "        for dirname in dirnames:\n",
    "            for filename in filenames:\n",
    "                path = dirpath + \"/\" + dirname + \"/\" + filename\n",
    "                if os.path.isfile(path):   \n",
    "                    with open(dirpath + \"/\" + dirname + \"/\" + filename, 'r') as f:\n",
    "                        read_data = f.readlines()\n",
    "                        if email_dir not in email_text:\n",
    "                            email_text[email_dir] = ''.join(read_data[15:])\n",
    "                        else:\n",
    "                            email_text[email_dir] += ''.join(read_data[15:])\n",
    "                \n",
    "#print email_text['white-s']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
